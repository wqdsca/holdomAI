"""
RTX 3080 Training Analysis for Poker Imitation Learning
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, í›ˆë ¨ ì‹œê°„, ì˜ˆìƒ ì •í™•ë„ ë¶„ì„
"""

import torch
import numpy as np
from pathlib import Path
import json
import time
from typing import Dict, Tuple
import matplotlib.pyplot as plt

class RTX3080TrainingAnalyzer:
    """RTX 3080ì—ì„œì˜ í›ˆë ¨ ê°€ëŠ¥ì„± ë¶„ì„"""
    
    def __init__(self):
        # RTX 3080 ì‚¬ì–‘
        self.gpu_specs = {
            'vram_gb': 10,
            'cuda_cores': 8704,
            'tensor_cores': 272,
            'base_clock_mhz': 1440,
            'boost_clock_mhz': 1710,
            'memory_bandwidth_gb_s': 760,
            'fp32_tflops': 29.77,
            'tensor_tflops': 238  # Mixed precision
        }
        
        # ëª¨ë¸ êµ¬ì„±
        self.model_configs = {
            'small': {
                'd_model': 128,
                'n_heads': 4,
                'n_layers': 3,
                'parameters': 2.1e6,
                'memory_mb': 25
            },
            'medium': {
                'd_model': 256,
                'n_heads': 8,
                'n_layers': 4,
                'parameters': 8.4e6,
                'memory_mb': 85
            },
            'large': {
                'd_model': 512,
                'n_heads': 8,
                'n_layers': 6,
                'parameters': 33.6e6,
                'memory_mb': 320
            },
            'xlarge': {
                'd_model': 1024,
                'n_heads': 16,
                'n_layers': 8,
                'parameters': 134.4e6,
                'memory_mb': 1280
            }
        }
        
        # ë°ì´í„°ì…‹ ì •ë³´
        self.dataset_info = {
            'total_hands': 21_605_687,
            'avg_actions_per_hand': 8.5,
            'total_actions': 21_605_687 * 8.5,
            'feature_dim': 800,
            'estimated_size_gb': 45.2
        }
    
    def calculate_memory_usage(
        self,
        model_config: str,
        batch_size: int,
        sequence_length: int = 20
    ) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        
        config = self.model_configs[model_config]
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ë©”ëª¨ë¦¬ (FP32)
        model_memory = config['parameters'] * 4 / (1024**2)  # MB
        
        # Optimizer ë©”ëª¨ë¦¬ (Adam: 2x parameters for momentum + variance)
        optimizer_memory = model_memory * 2
        
        # ë°°ì¹˜ ë°ì´í„° ë©”ëª¨ë¦¬
        input_size = batch_size * sequence_length * self.dataset_info['feature_dim'] * 4 / (1024**2)
        
        # Gradient ë©”ëª¨ë¦¬
        gradient_memory = model_memory
        
        # Activation ë©”ëª¨ë¦¬ (ì¶”ì •ì¹˜)
        activation_memory = batch_size * sequence_length * config['d_model'] * config['n_layers'] * 4 / (1024**2)
        
        total_memory_mb = (
            model_memory + 
            optimizer_memory + 
            input_size + 
            gradient_memory + 
            activation_memory + 
            200  # ê¸°íƒ€ ì˜¤ë²„í—¤ë“œ
        )
        
        return {
            'model_mb': model_memory,
            'optimizer_mb': optimizer_memory,
            'input_mb': input_size,
            'gradient_mb': gradient_memory,
            'activation_mb': activation_memory,
            'total_mb': total_memory_mb,
            'total_gb': total_memory_mb / 1024,
            'gpu_utilization': (total_memory_mb / 1024) / self.gpu_specs['vram_gb']
        }
    
    def estimate_training_time(
        self,
        model_config: str,
        batch_size: int,
        num_epochs: int,
        data_fraction: float = 1.0
    ) -> Dict:
        """í›ˆë ¨ ì‹œê°„ ì¶”ì •"""
        
        config = self.model_configs[model_config]
        total_samples = int(self.dataset_info['total_actions'] * data_fraction)
        
        # ë°°ì¹˜ë‹¹ ì—°ì‚°ëŸ‰ ì¶”ì • (FLOPS)
        sequence_length = 20
        d_model = config['d_model']
        n_heads = config['n_heads']
        n_layers = config['n_layers']
        
        # Transformer ì—°ì‚°ëŸ‰ (ê°„ë‹¨í•œ ì¶”ì •)
        attention_flops = batch_size * sequence_length**2 * d_model * n_heads * n_layers * 2
        ffn_flops = batch_size * sequence_length * d_model * (d_model * 4) * n_layers * 2
        output_flops = batch_size * d_model * 6 * 2  # 6 actions
        
        total_flops_per_batch = attention_flops + ffn_flops + output_flops
        
        # ë°°ì¹˜ ìˆ˜ ê³„ì‚°
        batches_per_epoch = total_samples // batch_size
        total_batches = batches_per_epoch * num_epochs
        total_flops = total_batches * total_flops_per_batch
        
        # ì²˜ë¦¬ ì‹œê°„ ì¶”ì • (Mixed Precision ì‚¬ìš©)
        effective_tflops = self.gpu_specs['tensor_tflops'] * 0.6  # ì‹¤ì œ íš¨ìœ¨ì„± ê³ ë ¤
        training_time_hours = (total_flops / 1e12) / effective_tflops / 3600
        
        # I/O ì˜¤ë²„í—¤ë“œ ì¶”ê°€ (20%)
        total_time_hours = training_time_hours * 1.2
        
        return {
            'total_samples': total_samples,
            'batches_per_epoch': batches_per_epoch,
            'total_batches': total_batches,
            'total_flops': total_flops,
            'compute_time_hours': training_time_hours,
            'total_time_hours': total_time_hours,
            'time_per_epoch_minutes': (total_time_hours * 60) / num_epochs,
            'samples_per_second': total_samples / (total_time_hours * 3600)
        }
    
    def predict_accuracy(
        self,
        model_config: str,
        data_fraction: float = 1.0,
        training_quality: str = 'good'  # 'poor', 'fair', 'good', 'excellent'
    ) -> Dict:
        """ì˜ˆìƒ ì •í™•ë„ ë¶„ì„"""
        
        config = self.model_configs[model_config]
        
        # ê¸°ë³¸ ì •í™•ë„ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
        base_accuracies = {
            'small': 0.62,
            'medium': 0.68,
            'large': 0.73,
            'xlarge': 0.76
        }
        
        base_acc = base_accuracies[model_config]
        
        # ë°ì´í„° ì–‘ì— ë”°ë¥¸ ë³´ì •
        data_boost = min(0.08 * np.log10(data_fraction * 10), 0.08)
        
        # í›ˆë ¨ í’ˆì§ˆì— ë”°ë¥¸ ë³´ì •
        quality_multipliers = {
            'poor': 0.85,
            'fair': 0.92,
            'good': 1.0,
            'excellent': 1.05
        }
        
        # ìµœì¢… ì •í™•ë„
        final_accuracy = (base_acc + data_boost) * quality_multipliers[training_quality]
        
        # ì•¡ì…˜ë³„ ì„¸ë¶€ ì •í™•ë„ ì¶”ì •
        action_accuracies = {
            'fold': final_accuracy + 0.05,  # ê°€ì¥ ì‰¬ìš´ ì•¡ì…˜
            'check': final_accuracy + 0.02,
            'call': final_accuracy,
            'bet': final_accuracy - 0.03,
            'raise': final_accuracy - 0.05,
            'all_in': final_accuracy - 0.08  # ê°€ì¥ ì–´ë ¤ìš´ ì•¡ì…˜
        }
        
        # í¬ì»¤ ìŠ¤íƒ€ì¼ ë©”íŠ¸ë¦­ ì¶”ì •
        estimated_metrics = {
            'overall_accuracy': final_accuracy,
            'action_accuracies': action_accuracies,
            'vpip': 0.22,  # ì¼ë°˜ì ì¸ íƒ€ì´íŠ¸-ì–´ê·¸ë ˆì‹œë¸Œ
            'pfr': 0.16,
            'aggression_factor': 2.8,
            'bet_sizing_mae': 0.15,  # íŒŸ ëŒ€ë¹„
            'convergence_epochs': max(10, 80 - config['parameters'] / 1e6)
        }
        
        return estimated_metrics
    
    def recommend_configuration(self) -> Dict:
        """ìµœì  êµ¬ì„± ì¶”ì²œ"""
        
        print("\n" + "="*70)
        print("RTX 3080 í¬ì»¤ AI í›ˆë ¨ ë¶„ì„ ê²°ê³¼")
        print("="*70)
        
        results = {}
        
        # ê° ëª¨ë¸ êµ¬ì„±ë³„ ë¶„ì„
        for model_name, config in self.model_configs.items():
            print(f"\nğŸ” {model_name.upper()} ëª¨ë¸ ë¶„ì„:")
            
            # ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
            optimal_batch = self.find_optimal_batch_size(model_name)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_info = self.calculate_memory_usage(model_name, optimal_batch)
            
            # í›ˆë ¨ ì‹œê°„ (100 ì—í¬í¬, ì „ì²´ ë°ì´í„°)
            time_info = self.estimate_training_time(model_name, optimal_batch, 100, 1.0)
            
            # ì •í™•ë„ ì˜ˆì¸¡
            accuracy_info = self.predict_accuracy(model_name, 1.0, 'good')
            
            # ì‹¤ìš©ì„± ì ìˆ˜ ê³„ì‚°
            practicality_score = self.calculate_practicality_score(
                memory_info, time_info, accuracy_info
            )
            
            results[model_name] = {
                'optimal_batch_size': optimal_batch,
                'memory_usage': memory_info,
                'training_time': time_info,
                'accuracy': accuracy_info,
                'practicality_score': practicality_score
            }
            
            # ì¶œë ¥
            print(f"   ğŸ“Š íŒŒë¼ë¯¸í„°: {config['parameters']/1e6:.1f}M")
            print(f"   ğŸ”‹ ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_info['total_gb']:.1f}GB ({memory_info['gpu_utilization']*100:.1f}%)")
            print(f"   â° í›ˆë ¨ ì‹œê°„: {time_info['total_time_hours']:.1f}ì‹œê°„")
            print(f"   ğŸ¯ ì˜ˆìƒ ì •í™•ë„: {accuracy_info['overall_accuracy']*100:.1f}%")
            print(f"   â­ ì‹¤ìš©ì„± ì ìˆ˜: {practicality_score:.1f}/10")
            
            # ê°€ëŠ¥/ë¶ˆê°€ëŠ¥ íŒì •
            if memory_info['gpu_utilization'] > 0.95:
                print(f"   âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ í›ˆë ¨ ë¶ˆê°€ëŠ¥")
            elif time_info['total_time_hours'] > 72:
                print(f"   âš ï¸  í›ˆë ¨ ì‹œê°„ì´ ë„ˆë¬´ ê¸¸ì–´ ë¹„ì‹¤ìš©ì ")
            else:
                print(f"   âœ… í›ˆë ¨ ê°€ëŠ¥")
        
        # ìµœê³  ì¶”ì²œ êµ¬ì„±
        best_config = max(results.keys(), key=lambda k: results[k]['practicality_score'])
        
        print(f"\nğŸ† ìµœì  ì¶”ì²œ êµ¬ì„±: {best_config.upper()}")
        print(f"   ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {results[best_config]['optimal_batch_size']}")
        print(f"   ì˜ˆìƒ í›ˆë ¨ ì‹œê°„: {results[best_config]['training_time']['total_time_hours']:.1f}ì‹œê°„")
        print(f"   ì˜ˆìƒ ì •í™•ë„: {results[best_config]['accuracy']['overall_accuracy']*100:.1f}%")
        
        return results
    
    def find_optimal_batch_size(self, model_config: str) -> int:
        """ë©”ëª¨ë¦¬ ì œì•½ í•˜ì—ì„œ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°"""
        batch_sizes = [8, 16, 32, 64, 128, 256]
        
        for batch_size in reversed(batch_sizes):
            memory_info = self.calculate_memory_usage(model_config, batch_size)
            if memory_info['gpu_utilization'] <= 0.85:  # 85% ì´í•˜ ì‚¬ìš©
                return batch_size
        
        return 8  # ìµœì†Œê°’
    
    def calculate_practicality_score(
        self,
        memory_info: Dict,
        time_info: Dict,
        accuracy_info: Dict
    ) -> float:
        """ì‹¤ìš©ì„± ì ìˆ˜ ê³„ì‚° (0-10)"""
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜ (0-3)
        if memory_info['gpu_utilization'] > 0.95:
            memory_score = 0
        elif memory_info['gpu_utilization'] > 0.85:
            memory_score = 1
        elif memory_info['gpu_utilization'] > 0.70:
            memory_score = 2
        else:
            memory_score = 3
        
        # ì‹œê°„ ì ìˆ˜ (0-3)
        hours = time_info['total_time_hours']
        if hours > 72:
            time_score = 0
        elif hours > 36:
            time_score = 1
        elif hours > 12:
            time_score = 2
        else:
            time_score = 3
        
        # ì •í™•ë„ ì ìˆ˜ (0-4)
        acc = accuracy_info['overall_accuracy']
        if acc > 0.75:
            acc_score = 4
        elif acc > 0.70:
            acc_score = 3
        elif acc > 0.65:
            acc_score = 2
        elif acc > 0.60:
            acc_score = 1
        else:
            acc_score = 0
        
        return memory_score + time_score + acc_score
    
    def create_training_schedule(self, model_config: str) -> Dict:
        """ë‹¨ê³„ë³„ í›ˆë ¨ ìŠ¤ì¼€ì¤„ ìƒì„±"""
        
        # ì ì§„ì  í›ˆë ¨ ì „ëµ
        schedule = {
            'phase_1': {
                'description': 'ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…',
                'data_fraction': 0.01,
                'epochs': 20,
                'batch_size': self.find_optimal_batch_size(model_config),
                'learning_rate': 1e-3
            },
            'phase_2': {
                'description': 'ì¤‘ê°„ ë°ì´í„°ì…‹ìœ¼ë¡œ ê²€ì¦',
                'data_fraction': 0.1,
                'epochs': 50,
                'batch_size': self.find_optimal_batch_size(model_config),
                'learning_rate': 5e-4
            },
            'phase_3': {
                'description': 'ì „ì²´ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì¢… í›ˆë ¨',
                'data_fraction': 1.0,
                'epochs': 100,
                'batch_size': self.find_optimal_batch_size(model_config),
                'learning_rate': 1e-4
            }
        }
        
        total_time = 0
        for phase_name, phase in schedule.items():
            time_info = self.estimate_training_time(
                model_config, 
                phase['batch_size'], 
                phase['epochs'], 
                phase['data_fraction']
            )
            phase['estimated_hours'] = time_info['total_time_hours']
            total_time += phase['estimated_hours']
        
        schedule['total_time_hours'] = total_time
        schedule['total_time_days'] = total_time / 24
        
        return schedule


if __name__ == "__main__":
    analyzer = RTX3080TrainingAnalyzer()
    
    # ë¶„ì„ ì‹¤í–‰
    results = analyzer.recommend_configuration()
    
    # ì¶”ì²œ êµ¬ì„±ì— ëŒ€í•œ ìƒì„¸ ìŠ¤ì¼€ì¤„
    best_model = 'medium'  # ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ì‹¤ìš©ì 
    schedule = analyzer.create_training_schedule(best_model)
    
    print(f"\nğŸ“… {best_model.upper()} ëª¨ë¸ í›ˆë ¨ ìŠ¤ì¼€ì¤„:")
    for phase_name, phase in schedule.items():
        if phase_name.startswith('phase'):
            print(f"   {phase_name}: {phase['description']}")
            print(f"      ë°ì´í„°: {phase['data_fraction']*100:.1f}%, ì—í¬í¬: {phase['epochs']}")
            print(f"      ì˜ˆìƒ ì‹œê°„: {phase['estimated_hours']:.1f}ì‹œê°„")
    
    print(f"\nì´ í›ˆë ¨ ì‹œê°„: {schedule['total_time_hours']:.1f}ì‹œê°„ ({schedule['total_time_days']:.1f}ì¼)")