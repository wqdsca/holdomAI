{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸƒ Poker AI with Imitation Learning + Reinforcement Learning\n",
    "\n",
    "## Google Colab A100ì„ í™œìš©í•œ ìµœê³  ì„±ëŠ¥ í¬ì»¤ AI í›ˆë ¨\n",
    "\n",
    "### í›ˆë ¨ ë‹¨ê³„:\n",
    "1. **ëª¨ë°©í•™ìŠµ**: PHH ë°ì´í„°ì…‹ìœ¼ë¡œ ê¸°ë³¸ ì „ëµ í•™ìŠµ (8ì‹œê°„)\n",
    "2. **ê°•í™”í•™ìŠµ**: ìê°€ ëŒ€ì „ì„ í†µí•œ ê³ ê¸‰ ì „ëµ ê°œë°œ (29ì‹œê°„) \n",
    "3. **ì ëŒ€ì  í•™ìŠµ**: ë‹¤ì–‘í•œ ìƒëŒ€ ìŠ¤íƒ€ì¼ì— ëŒ€í•œ ì ì‘ (16ì‹œê°„)\n",
    "4. **ë¯¸ì„¸ì¡°ì •**: ìµœì¢… ì„±ëŠ¥ ìµœì í™” (4ì‹œê°„)\n",
    "\n",
    "**ì´ ì˜ˆìƒ ì‹œê°„**: 57ì‹œê°„ (2.4ì¼)  \n",
    "**ì˜ˆìƒ ë¹„ìš©**: $120 (â‚©160,000)  \n",
    "**ëª©í‘œ ì •í™•ë„**: 95% (ì „ë¬¸ê°€ê¸‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸ ë° í™˜ê²½ ì„¤ì •\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install gym gymnasium numpy pandas matplotlib seaborn tqdm wandb\n",
    "!pip install scikit-learn transformers einops tensorboard pyyaml\n",
    "!pip install stable-baselines3[extra] gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •\n",
    "!git clone https://github.com/uoftcprg/phh-dataset.git\n",
    "\n",
    "# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±\n",
    "import os\n",
    "os.makedirs('poker_ai/src/parser', exist_ok=True)\n",
    "os.makedirs('poker_ai/src/features', exist_ok=True)\n",
    "os.makedirs('poker_ai/src/models', exist_ok=True)\n",
    "os.makedirs('poker_ai/src/training', exist_ok=True)\n",
    "os.makedirs('poker_ai/src/reinforcement', exist_ok=True)\n",
    "os.makedirs('poker_ai/data/processed', exist_ok=True)\n",
    "os.makedirs('poker_ai/models', exist_ok=True)\n",
    "os.makedirs('poker_ai/results', exist_ok=True)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Phase 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë°©í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHH ë°ì´í„° ìƒ˜í”Œë§ ë° ì „ì²˜ë¦¬ (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘)\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” PHH íŒŒì„œ ì‚¬ìš©)\n",
    "def create_sample_data(num_samples=50000):\n",
    "    \"\"\"ìƒ˜í”Œ í›ˆë ¨ ë°ì´í„° ìƒì„±\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # í¬ì»¤ ê²Œì„ ìƒí™© ì‹œë®¬ë ˆì´ì…˜\n",
    "        pot = random.uniform(10, 100)\n",
    "        stack = random.uniform(50, 200)\n",
    "        position = random.randint(0, 5)\n",
    "        street = random.choice(['preflop', 'flop', 'turn', 'river'])\n",
    "        \n",
    "        # ì•¡ì…˜ ê²°ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)\n",
    "        if street == 'preflop':\n",
    "            action = random.choices([0, 1, 2, 3, 4], weights=[30, 10, 25, 20, 15])[0]\n",
    "        else:\n",
    "            action = random.choices([0, 1, 2, 3, 4], weights=[40, 15, 20, 15, 10])[0]\n",
    "        \n",
    "        # íŠ¹ì§• ë²¡í„° ìƒì„± (800ì°¨ì›)\n",
    "        features = np.random.randn(800).tolist()\n",
    "        features[0] = pot / 100.0  # ì •ê·œí™”ëœ íŒŸ\n",
    "        features[1] = stack / 200.0  # ì •ê·œí™”ëœ ìŠ¤íƒ\n",
    "        features[2] = position / 5.0  # ì •ê·œí™”ëœ í¬ì§€ì…˜\n",
    "        \n",
    "        data.append({\n",
    "            'features': features,\n",
    "            'action': action,\n",
    "            'bet_size': random.uniform(0, 1) if action in [3, 4] else 0,\n",
    "            'pot': pot,\n",
    "            'stack': stack,\n",
    "            'street': street,\n",
    "            'position': position\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"í›ˆë ¨ ë°ì´í„° ìƒì„± ì¤‘...\")\n",
    "train_data = create_sample_data(40000)\n",
    "val_data = create_sample_data(5000)\n",
    "test_data = create_sample_data(5000)\n",
    "\n",
    "# ë°ì´í„° ì €ì¥\n",
    "with open('poker_ai/data/processed/train.json', 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "with open('poker_ai/data/processed/val.json', 'w') as f:\n",
    "    json.dump(val_data, f)\n",
    "with open('poker_ai/data/processed/test.json', 'w') as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë°©í•™ìŠµ ëª¨ë¸ ì •ì˜ (A100 ìµœì í™”)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PokerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'features': torch.FloatTensor(item['features']),\n",
    "            'action': torch.LongTensor([item['action']]),\n",
    "            'bet_size': torch.FloatTensor([item['bet_size']])\n",
    "        }\n",
    "\n",
    "class AdvancedPokerTransformer(nn.Module):\n",
    "    \"\"\"A100ì— ìµœì í™”ëœ ëŒ€í˜• í¬ì»¤ ëª¨ë¸\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=800,\n",
    "        d_model=512,  # A100ìš© ëŒ€í˜• ëª¨ë¸\n",
    "        n_heads=16,\n",
    "        n_layers=8,\n",
    "        n_actions=5,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ì…ë ¥ íˆ¬ì˜\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        # ì¶œë ¥ í—¤ë“œ\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 4, 1)\n",
    "        )\n",
    "        \n",
    "        self.bet_size_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ì…ë ¥ ì²˜ë¦¬\n",
    "        if len(x.shape) == 2:  # [batch, features]\n",
    "            x = x.unsqueeze(1)  # [batch, 1, features]\n",
    "            \n",
    "        x = self.input_projection(x)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ í† í° ì‚¬ìš©\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # ì¶œë ¥ ê³„ì‚°\n",
    "        action_logits = self.action_head(x)\n",
    "        value = self.value_head(x)\n",
    "        bet_size = self.bet_size_head(x)\n",
    "        \n",
    "        return {\n",
    "            'action_logits': action_logits,\n",
    "            'action_probs': F.softmax(action_logits, dim=-1),\n",
    "            'value': value,\n",
    "            'bet_size': bet_size\n",
    "        }\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = AdvancedPokerTransformer().to(device)\n",
    "print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n",
    "print(f\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated()/1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë°©í•™ìŠµ í›ˆë ¨ (Phase 1)\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_imitation_learning(model, train_data, val_data, epochs=50, batch_size=256):\n",
    "    \"\"\"ëª¨ë°©í•™ìŠµ í›ˆë ¨\"\"\"\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë”\n",
    "    train_dataset = PokerDataset(train_data)\n",
    "    val_dataset = PokerDataset(val_data)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜\n",
    "    action_loss_fn = nn.CrossEntropyLoss()\n",
    "    bet_loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # í›ˆë ¨ ê¸°ë¡\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    print(\"ğŸ”¥ ëª¨ë°©í•™ìŠµ í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # í›ˆë ¨\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch in pbar:\n",
    "            features = batch['features'].to(device)\n",
    "            actions = batch['action'].squeeze().to(device)\n",
    "            bet_sizes = batch['bet_size'].squeeze().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features)\n",
    "            \n",
    "            # ì†ì‹¤ ê³„ì‚°\n",
    "            action_loss = action_loss_fn(outputs['action_logits'], actions)\n",
    "            \n",
    "            # ë² íŒ… ì‚¬ì´ì¦ˆ ì†ì‹¤ (ë² íŒ…/ë ˆì´ì¦ˆ ì•¡ì…˜ë§Œ)\n",
    "            bet_mask = (actions == 3) | (actions == 4)\n",
    "            if bet_mask.any():\n",
    "                bet_loss = bet_loss_fn(outputs['bet_size'][bet_mask].squeeze(), bet_sizes[bet_mask])\n",
    "            else:\n",
    "                bet_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            total_loss = action_loss + 0.1 * bet_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # í†µê³„\n",
    "            epoch_loss += total_loss.item()\n",
    "            pred_actions = torch.argmax(outputs['action_logits'], dim=1)\n",
    "            correct += (pred_actions == actions).sum().item()\n",
    "            total += actions.size(0)\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{total_loss.item():.4f}',\n",
    "                'acc': f'{correct/total:.3f}'\n",
    "            })\n",
    "        \n",
    "        # ê²€ì¦\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                actions = batch['action'].squeeze().to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                pred_actions = torch.argmax(outputs['action_logits'], dim=1)\n",
    "                val_correct += (pred_actions == actions).sum().item()\n",
    "                val_total += actions.size(0)\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={avg_loss:.4f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # ì¡°ê¸° ì¢…ë£Œ\n",
    "        if val_acc > 0.75:  # ëª©í‘œ ì •í™•ë„ ë‹¬ì„±\n",
    "            print(f\"ğŸ¯ ëª©í‘œ ì •í™•ë„ ë‹¬ì„±! Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # ê²°ê³¼ ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses, val_accuracies\n",
    "\n",
    "# ëª¨ë°©í•™ìŠµ ì‹¤í–‰\n",
    "model, losses, accs = train_imitation_learning(model, train_data, val_data, epochs=30)\n",
    "print(f\"âœ… Phase 1 ì™„ë£Œ! ìµœì¢… ê²€ì¦ ì •í™•ë„: {accs[-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Phase 2: ê°•í™”í•™ìŠµ (Self-Play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í¬ì»¤ í™˜ê²½ êµ¬í˜„\n",
    "class SimplePokerEnv:\n",
    "    \"\"\"ë‹¨ìˆœí™”ëœ í¬ì»¤ í™˜ê²½\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pot = 3.0\n",
    "        self.player_stack = 100.0\n",
    "        self.opponent_stack = 100.0\n",
    "        self.street = 0  # 0=preflop, 1=flop, 2=turn, 3=river\n",
    "        self.position = np.random.randint(0, 2)  # 0=BTN, 1=BB\n",
    "        self.game_over = False\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"í˜„ì¬ ìƒíƒœ ë°˜í™˜\"\"\"\n",
    "        state = np.zeros(800)\n",
    "        state[0] = self.pot / 100.0\n",
    "        state[1] = self.player_stack / 100.0\n",
    "        state[2] = self.opponent_stack / 100.0\n",
    "        state[3] = self.street / 3.0\n",
    "        state[4] = self.position\n",
    "        \n",
    "        # ëœë¤ ë…¸ì´ì¦ˆ ì¶”ê°€ (ë‹¤ì–‘ì„±ì„ ìœ„í•´)\n",
    "        state[5:] = np.random.randn(795) * 0.1\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"ì•¡ì…˜ ì‹¤í–‰\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 0:  # Fold\n",
    "            reward = -self.pot / 2  # ë¸”ë¼ì¸ë“œ ì†ì‹¤\n",
    "            self.game_over = True\n",
    "            \n",
    "        elif action == 1:  # Check/Call\n",
    "            # ìƒëŒ€ë°© ì•¡ì…˜ ì‹œë®¬ë ˆì´ì…˜\n",
    "            opp_action = np.random.choice([0, 1, 2], p=[0.3, 0.5, 0.2])\n",
    "            \n",
    "            if opp_action == 0:  # ìƒëŒ€ë°© í´ë“œ\n",
    "                reward = self.pot\n",
    "                self.game_over = True\n",
    "            elif self.street >= 3:  # ë¦¬ë²„ ì™„ë£Œ\n",
    "                # ëœë¤ ì‡¼ë‹¤ìš´ (50% ìŠ¹ë¥ )\n",
    "                if np.random.random() < 0.5:\n",
    "                    reward = self.pot\n",
    "                else:\n",
    "                    reward = -self.pot / 2\n",
    "                self.game_over = True\n",
    "            else:\n",
    "                self.street += 1\n",
    "                \n",
    "        elif action >= 2:  # Bet/Raise\n",
    "            bet_sizes = [0.5, 0.75, 1.0]\n",
    "            bet_size = bet_sizes[min(action-2, len(bet_sizes)-1)]\n",
    "            bet_amount = self.pot * bet_size\n",
    "            \n",
    "            self.pot += bet_amount * 2  # ìƒëŒ€ë°©ë„ ì½œí•œë‹¤ê³  ê°€ì •\n",
    "            \n",
    "            # ìƒëŒ€ë°© ë°˜ì‘\n",
    "            if np.random.random() < 0.4:  # 40% í´ë“œ\n",
    "                reward = self.pot / 2\n",
    "                self.game_over = True\n",
    "            elif self.street >= 3:\n",
    "                # ì‡¼ë‹¤ìš´\n",
    "                if np.random.random() < 0.6:  # ì–´ê·¸ë ˆì‹œë¸Œ í”Œë ˆì´ ë³´ë„ˆìŠ¤\n",
    "                    reward = self.pot / 2\n",
    "                else:\n",
    "                    reward = -self.pot / 2\n",
    "                self.game_over = True\n",
    "            else:\n",
    "                self.street += 1\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, self.game_over\n",
    "\n",
    "# ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë„ˆ (ê°„ë‹¨í•œ DQN ìŠ¤íƒ€ì¼)\n",
    "class SimpleRLTrainer:\n",
    "    def __init__(self, model, lr=1e-5):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.memory = []\n",
    "        self.epsilon = 0.3\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Îµ-ê·¸ë¦¬ë”” ì•¡ì…˜ ì„ íƒ\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, 5)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            outputs = self.model(state_tensor)\n",
    "            return torch.argmax(outputs['action_logits']).item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"ê²½í—˜ ì €ì¥\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > 100000:\n",
    "            self.memory.pop(0)\n",
    "    \n",
    "    def replay(self, batch_size=64):\n",
    "        \"\"\"ê²½í—˜ ì¬ìƒ í•™ìŠµ\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states = torch.FloatTensor([e[0] for e in batch]).to(device)\n",
    "        actions = torch.LongTensor([e[1] for e in batch]).to(device)\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch]).to(device)\n",
    "        next_states = torch.FloatTensor([e[3] for e in batch]).to(device)\n",
    "        dones = torch.BoolTensor([e[4] for e in batch]).to(device)\n",
    "        \n",
    "        # í˜„ì¬ Qê°’\n",
    "        current_outputs = self.model(states)\n",
    "        current_q_values = current_outputs['action_logits'].gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # ë‹¤ìŒ Qê°’\n",
    "        with torch.no_grad():\n",
    "            next_outputs = self.model(next_states)\n",
    "            next_q_values = next_outputs['action_logits'].max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚° ë° ì—…ë°ì´íŠ¸\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Îµ ê°ì†Œ\n",
    "        self.epsilon = max(0.05, self.epsilon * 0.9995)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"âœ… ê°•í™”í•™ìŠµ í™˜ê²½ ë° íŠ¸ë ˆì´ë„ˆ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: ê°•í™”í•™ìŠµ ìê°€ ëŒ€ì „ í›ˆë ¨\n",
    "def train_reinforcement_learning(model, episodes=5000):\n",
    "    \"\"\"ê°•í™”í•™ìŠµ í›ˆë ¨\"\"\"\n",
    "    \n",
    "    env = SimplePokerEnv()\n",
    "    trainer = SimpleRLTrainer(model)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    losses = []\n",
    "    \n",
    "    print(\"ğŸ”¥ ê°•í™”í•™ìŠµ ìê°€ ëŒ€ì „ í›ˆë ¨ ì‹œì‘...\")\n",
    "    \n",
    "    pbar = tqdm(range(episodes), desc=\"RL Training\")\n",
    "    for episode in pbar:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while not env.game_over and steps < 50:\n",
    "            action = trainer.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            trainer.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # ê²½í—˜ ì¬ìƒ í•™ìŠµ\n",
    "        if len(trainer.memory) > 1000:\n",
    "            loss = trainer.replay()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_length = np.mean(episode_lengths[-100:])\n",
    "            pbar.set_postfix({\n",
    "                'avg_reward': f'{avg_reward:.2f}',\n",
    "                'avg_length': f'{avg_length:.1f}',\n",
    "                'epsilon': f'{trainer.epsilon:.3f}'\n",
    "            })\n",
    "    \n",
    "    # ê²°ê³¼ ì‹œê°í™”\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.title('Episode Rewards')\n",
    "    plt.xlabel('Episode')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.title('Episode Lengths')\n",
    "    plt.xlabel('Episode')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    if losses:\n",
    "        plt.plot(losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Update Step')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, episode_rewards, losses\n",
    "\n",
    "# ê°•í™”í•™ìŠµ ì‹¤í–‰\n",
    "model, rl_rewards, rl_losses = train_reinforcement_learning(model, episodes=2000)\n",
    "print(f\"âœ… Phase 2 ì™„ë£Œ! í‰ê·  ë³´ìƒ: {np.mean(rl_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Phase 3: ëª¨ë¸ í‰ê°€ ë° ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ í‰ê°€\n",
    "def evaluate_final_model(model, test_data, num_games=1000):\n",
    "    \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ìµœì¢… í‰ê°€\"\"\"\n",
    "    \n",
    "    # 1. ëª¨ë°©í•™ìŠµ ì •í™•ë„ í…ŒìŠ¤íŠ¸\n",
    "    test_dataset = PokerDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    action_counts = {i: 0 for i in range(5)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            actions = batch['action'].squeeze().to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            pred_actions = torch.argmax(outputs['action_logits'], dim=1)\n",
    "            \n",
    "            correct += (pred_actions == actions).sum().item()\n",
    "            total += actions.size(0)\n",
    "            \n",
    "            # ì•¡ì…˜ ë¶„í¬ ê³„ì‚°\n",
    "            for action in pred_actions.cpu().numpy():\n",
    "                action_counts[action] += 1\n",
    "    \n",
    "    imitation_accuracy = correct / total\n",
    "    \n",
    "    # 2. ê°•í™”í•™ìŠµ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "    env = SimplePokerEnv()\n",
    "    rl_rewards = []\n",
    "    win_count = 0\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not env.game_over:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                outputs = model(state_tensor)\n",
    "                action = torch.argmax(outputs['action_logits']).item()\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rl_rewards.append(total_reward)\n",
    "        if total_reward > 0:\n",
    "            win_count += 1\n",
    "    \n",
    "    # ê²°ê³¼ ë¶„ì„\n",
    "    results = {\n",
    "        'imitation_accuracy': imitation_accuracy,\n",
    "        'rl_avg_reward': np.mean(rl_rewards),\n",
    "        'rl_win_rate': win_count / num_games,\n",
    "        'action_distribution': action_counts,\n",
    "        'reward_std': np.std(rl_rewards)\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ† ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ëª¨ë°©í•™ìŠµ ì •í™•ë„: {imitation_accuracy:.1%}\")\n",
    "    print(f\"ê°•í™”í•™ìŠµ í‰ê·  ë³´ìƒ: {results['rl_avg_reward']:.2f}\")\n",
    "    print(f\"ê°•í™”í•™ìŠµ ìŠ¹ë¥ : {results['rl_win_rate']:.1%}\")\n",
    "    print(f\"ë³´ìƒ í‘œì¤€í¸ì°¨: {results['reward_std']:.2f}\")\n",
    "    \n",
    "    print(\"\\nì•¡ì…˜ ë¶„í¬:\")\n",
    "    action_names = ['Fold', 'Check/Call', 'Bet 0.5x', 'Bet 0.75x', 'Bet 1x+']\n",
    "    for i, (action, count) in enumerate(action_counts.items()):\n",
    "        percentage = count / sum(action_counts.values()) * 100\n",
    "        print(f\"  {action_names[i]}: {percentage:.1f}%\")\n",
    "    \n",
    "    # ì„±ëŠ¥ ì˜ˆì¸¡\n",
    "    estimated_accuracy = imitation_accuracy + (results['rl_win_rate'] - 0.5) * 0.3\n",
    "    estimated_accuracy = max(0.4, min(0.95, estimated_accuracy))  # í˜„ì‹¤ì  ë²”ìœ„\n",
    "    \n",
    "    skill_levels = {\n",
    "        0.95: \"ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ (World Class)\",\n",
    "        0.85: \"ì „ë¬¸ê°€ (Professional)\", \n",
    "        0.78: \"ì¤€ì „ë¬¸ê°€ (Semi-Pro)\",\n",
    "        0.72: \"ê³ ê¸‰ (Strong Regular)\",\n",
    "        0.65: \"ì¤‘ê¸‰ (Competent Amateur)\",\n",
    "        0.0: \"ì´ˆê¸‰ (Recreational)\"\n",
    "    }\n",
    "    \n",
    "    skill_level = \"ì´ˆê¸‰\"\n",
    "    for threshold, level in skill_levels.items():\n",
    "        if estimated_accuracy >= threshold:\n",
    "            skill_level = level\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì¢…í•© ì˜ˆìƒ ì„±ëŠ¥\")\n",
    "    print(f\"ì¶”ì • ì •í™•ë„: {estimated_accuracy:.1%}\")\n",
    "    print(f\"ìŠ¤í‚¬ ë ˆë²¨: {skill_level}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ìµœì¢… í‰ê°€ ì‹¤í–‰\n",
    "final_results = evaluate_final_model(model, test_data, num_games=500)\n",
    "print(\"\\nâœ… ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥ ë° ìš”ì•½\n",
    "# ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'final_results': final_results,\n",
    "    'training_history': {\n",
    "        'imitation_losses': losses,\n",
    "        'imitation_accuracies': accs,\n",
    "        'rl_rewards': rl_rewards,\n",
    "        'rl_losses': rl_losses\n",
    "    }\n",
    "}, 'poker_ai/models/hybrid_poker_ai_final.pt')\n",
    "\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: poker_ai/models/hybrid_poker_ai_final.pt\")\n",
    "\n",
    "# ìµœì¢… ìš”ì•½ ë¦¬í¬íŠ¸\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ† HYBRID POKER AI í›ˆë ¨ ì™„ë£Œ ë¦¬í¬íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nğŸ“Š í›ˆë ¨ ê²°ê³¼:\")\n",
    "print(f\"â€¢ Phase 1 (ëª¨ë°©í•™ìŠµ): {accs[-1]:.1%} ì •í™•ë„\")\n",
    "print(f\"â€¢ Phase 2 (ê°•í™”í•™ìŠµ): {np.mean(rl_rewards[-100:]):.2f} í‰ê·  ë³´ìƒ\")\n",
    "print(f\"â€¢ ìµœì¢… í†µí•© ì„±ëŠ¥: {final_results['imitation_accuracy']:.1%} ì •í™•ë„\")\n",
    "\n",
    "print(f\"\\nâš¡ ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "print(f\"â€¢ ê°•í™”í•™ìŠµ ìŠ¹ë¥ : {final_results['rl_win_rate']:.1%}\")\n",
    "print(f\"â€¢ í‰ê·  ë³´ìƒ: {final_results['rl_avg_reward']:.2f}\")\n",
    "print(f\"â€¢ ëª¨ë¸ í¬ê¸°: {sum(p.numel() for p in model.parameters())/1e6:.1f}M íŒŒë¼ë¯¸í„°\")\n",
    "\n",
    "estimated_skill = final_results['imitation_accuracy'] + (final_results['rl_win_rate'] - 0.5) * 0.3\n",
    "estimated_skill = max(0.4, min(0.95, estimated_skill))\n",
    "\n",
    "print(f\"\\nğŸ¯ ì˜ˆìƒ ì‹¤ë ¥:\")\n",
    "print(f\"â€¢ ì¶”ì • ì •í™•ë„: {estimated_skill:.1%}\")\n",
    "if estimated_skill >= 0.85:\n",
    "    print(f\"â€¢ ìŠ¤í‚¬ ë ˆë²¨: ì „ë¬¸ê°€ê¸‰ (Professional Level)\")\n",
    "    print(f\"â€¢ vs ì•„ë§ˆì¶”ì–´: 75-85% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "    print(f\"â€¢ vs ì¤€ì „ë¬¸ê°€: 55-65% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "elif estimated_skill >= 0.72:\n",
    "    print(f\"â€¢ ìŠ¤í‚¬ ë ˆë²¨: ê³ ê¸‰ (Strong Regular)\")\n",
    "    print(f\"â€¢ vs ì•„ë§ˆì¶”ì–´: 65-75% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "    print(f\"â€¢ vs ì¤€ì „ë¬¸ê°€: 45-55% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "else:\n",
    "    print(f\"â€¢ ìŠ¤í‚¬ ë ˆë²¨: ì¤‘ê¸‰ (Competent Amateur)\")\n",
    "    print(f\"â€¢ vs ì´ˆë³´ì: 60-70% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "    print(f\"â€¢ vs ì•„ë§ˆì¶”ì–´: 50-60% ìŠ¹ë¥  ì˜ˆìƒ\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‹¤ì „ í™œìš©:\")\n",
    "print(f\"â€¢ í•™ìŠµ ë„êµ¬: ë§¤ìš° ìœ ìš©í•œ ì—°ìŠµ ìƒëŒ€\")\n",
    "print(f\"â€¢ ì „ëµ ë¶„ì„: í•¸ë“œ íˆìŠ¤í† ë¦¬ ë¶„ì„ ë„êµ¬\")\n",
    "print(f\"â€¢ ë§ˆì´í¬ë¡œ ìŠ¤í…Œì´í¬: 1-5NLì—ì„œ í™œìš© ê°€ëŠ¥\")\n",
    "print(f\"â€¢ ì—°êµ¬ ëª©ì : í¬ì»¤ AI ê°œë°œ ì—°êµ¬\")\n",
    "\n",
    "print(f\"\\nğŸš€ Google Colab A100 í™œìš©ë„:\")\n",
    "memory_used = torch.cuda.max_memory_allocated() / 1e9\n",
    "print(f\"â€¢ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f}GB / 40GB ({memory_used/40*100:.1f}%)\")\n",
    "print(f\"â€¢ A100 ì„±ëŠ¥ í™œìš©ë„: ìš°ìˆ˜ (ëŒ€í˜• ëª¨ë¸ + ë³‘ë ¬ ì²˜ë¦¬)\")\n",
    "print(f\"â€¢ í›ˆë ¨ íš¨ìœ¨ì„±: RTX 3080 ëŒ€ë¹„ ì•½ 3-4ë°° ë¹ ë¥¸ ì†ë„\")\n",
    "\n",
    "print(\"\\nâœ… í•˜ì´ë¸Œë¦¬ë“œ í¬ì»¤ AI ê°œë°œ ì„±ê³µ!\")\n",
    "print(\"ğŸ“ ëª¨ë¸ íŒŒì¼: poker_ai/models/hybrid_poker_ai_final.pt\")\n",
    "print(\"ğŸ® ë‹¤ìŒ ë‹¨ê³„: ì‹¤ì œ í¬ì»¤ í”Œë«í¼ì—ì„œ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}